import schedule
import time
import requests
from pymongo import MongoClient

# âœ… Connect to MongoDB
client = MongoClient("mongodb://localhost:27017/")
db = client["scraped_data"]
posts_collection = db["posts"]
posts_details_collection = db["postsDetails"]

# âœ… Configuration
SCRAPE_API = "http://127.0.0.1:5000/scrape-post"  # Replace with your actual API endpoint

def scrape_and_store():
    """Fetches posts from 'posts' collection and scrapes only new ones."""
    all_posts = list(posts_collection.find({}, {"url": 1, "_id": 0, "id": 1}))
    post_count = 0  # Counter for saved posts
    skipped_count = 0  # Counter for skipped posts

    if not all_posts:
        print("âœ… No posts found in the database. Exiting...")
        schedule.clear()
        return

    for post in all_posts:
        post_url = post.get("url")
        post_id = post.get("id")

        if not post_url or not post_id:
            print("âš ï¸ Skipping post with missing URL or post ID")
            skipped_count += 1
            continue

        # âœ… Check if already exists in `postsDetails`
        if posts_details_collection.find_one({"post_collection_id": post_id}):
            print(f"âš ï¸ Post already exists in postsDetails, skipping: {post_id}")
            skipped_count += 1
            continue  # Move to the next post

        print(f"ðŸ”„ Scraping post: {post_url} && postId: {post_id}")

        try:
            response = requests.get(f"{SCRAPE_API}?url={post_url}")

            # ðŸš€ Debugging API Response
            print(f"ðŸ” Response Status: {response.status_code}")
            print(f"ðŸ“œ Response Headers: {response.headers}")

            if response.status_code in [200, 201]:  # âœ… Handle both success cases
                post_data = response.json()
                print(f"ðŸ“Š Scraped Data: {post_data}")

                if "data" in post_data and "post_id" in post_data["data"]:
                    inserted = posts_details_collection.insert_one(post_data)
                    post_count += 1  # Increment counter
                    print(f"âœ… Post {post_count} saved with ID: {inserted.inserted_id}")
                else:
                    print(f"âš ï¸ Scraped data does not contain a valid URL: {post_url}")

            elif response.status_code == 500:
                print(f"âŒ API request failed with 500 INTERNAL SERVER ERROR for {post_url}")
                print(f"ðŸ›‘ Error Message: {response.text}")  # âœ… Print full error message from the API

            else:
                print(f"âŒ API request failed for {post_url}, Status Code: {response.status_code}, Response: {response.text}")

        except requests.exceptions.RequestException as e:
            print(f"ðŸš¨ Network Error: {e}")  # âœ… Print network-related errors
        except Exception as e:
            print(f"âŒ Unexpected Error: {e}")  # âœ… Catch unexpected Python errors

    # âœ… Check if all posts are processed
    total_posts = len(all_posts)
    if post_count + skipped_count >= total_posts:
        print(f"ðŸŽ‰ All {total_posts} posts have been processed! Stopping the script.")
        schedule.clear()

# âœ… Schedule the script to run every 1 second
schedule.every(1).seconds.do(scrape_and_store)

print("ðŸš€ Auto-scraping started! Calling API every 1 second...")

while schedule.jobs:
    schedule.run_pending()
    time.sleep(1)  # Prevent excessive CPU usage

print("âœ… Scraping process completed. Exiting script.")